{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"\n",
    "    y : ソフトマックス関数の出力\n",
    "        y.shape=(k,)またはy.shape=(N,k)\n",
    "    t : 正解ラベル(ワンホット表現)\n",
    "        t.shape=(k,)またはt.shape=(N,k)\n",
    "    \"\"\"\n",
    "    if y.ndim==1:\n",
    "        t = t.reshape(1,-1)\n",
    "        y = y.reshape(1,-1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    # ↓参照:[1]\n",
    "    return -np.sum(t*np.log(y + delta))/ batch_size\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] -np.sum(t*np.log(y + delta))/ batch_size\n",
    "- バッチ版の交差エントロピー誤差を表している（式は以下で表される）\n",
    "  $\n",
    "  -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\rm{log}y_{nk}\n",
    "  $\n",
    "  - $N$：データ数\n",
    "  - $t_{nk}$：n個目のデータのk番目の正解ラベル\n",
    "  - $y_{nk}$：n個目のデータのk番目のニューラルネットワーク出力\n",
    "  - ※バッチ内のデータで損失の合計をとっている\n",
    "  - ※マイナスの符号が付くのは誤差関数を正に保つため。つまり，$y_{nk}$は0~1の確率をとるため，logの値は0~マイナス∞の値をとるため，それを＋符号に変換する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # ソフトマックス関数の出力を格納するインスタンス変数\n",
    "        self.t = None # 正解ラベルを格納するインスタンス変数\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y-self.t)/batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    x = np.arange(2*3).reshape(2, 3)\n",
    "    print(\"x\\n\", x)   \n",
    "    \n",
    "    t = np.arange(2).reshape(-1, 1)\n",
    "    print(\"t\\n\", t)   \n",
    "    \n",
    "    sl = SoftmaxWithLoss()\n",
    "    \n",
    "    loss = sl.forward(x,t)    \n",
    "    print(\"loss\\n\", loss)\n",
    "    \n",
    "    dx = sl.backward()    \n",
    "    print(\"dx\\n\", dx)       "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
